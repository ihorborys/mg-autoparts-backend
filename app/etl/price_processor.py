import os
import re
import gzip
import shutil
import yaml
import ftplib
from datetime import datetime
from typing import Tuple, List, Dict, Any, Optional
from pathlib import Path


import pandas as pd
# --- –Ü–º–ø–æ—Ä—Ç text –¥–ª—è –±–µ–∑–ø–µ—á–Ω–∏—Ö SQL-–∑–∞–ø–∏—Ç—ñ–≤ ---
from sqlalchemy import create_engine, text

from app.services.paths import TEMP_DIR
from app.services.storage import StorageClient

from re import sub


# ----------------------- FTP / unzip -----------------------

def download_file_from_ftp(remote_path: str, local_path: Path, supplier: str) -> None:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ñ–∞–π–ª –∑ FTP, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –¥–∏–Ω–∞–º—ñ—á–Ω—ñ —Å–µ–∫—Ä–µ—Ç–∏ –∑ .env
    –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ñ–º–µ–Ω—ñ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ (–Ω–∞–ø—Ä. AUTOPARTNER_FTP_HOST).
    """
    # 1) –ì–æ—Ç—É—î–º–æ –ø—Ä–µ—Ñ—ñ–∫—Å –¥–ª—è –ø–æ—à—É–∫—É –≤ .env (–Ω–∞–ø—Ä. "AUTOPARTNER")
    prefix = supplier.upper().replace(" ", "_")

    # 2) –í–∏—Ç—è–≥—É—î–º–æ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è —Ü—å–æ–≥–æ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞
    host = os.getenv(f"{prefix}_FTP_HOST")
    user = os.getenv(f"{prefix}_FTP_USER")
    pwd = os.getenv(f"{prefix}_FTP_PASS")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—â–æ –≤ .env –∑–∞–±—É–ª–∏ –ø—Ä–æ–ø–∏—Å–∞—Ç–∏ –¥–∞–Ω—ñ –¥–ª—è —Ü—å–æ–≥–æ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞
    if not all([host, user, pwd]):
        raise RuntimeError(f"Credentials for {prefix} are missing in .env. "
                           f"Please add {prefix}_FTP_HOST, {prefix}_FTP_USER, {prefix}_FTP_PASS.")

    print(f"[INFO] Connecting to FTP for {prefix} ({host})...")

    # –î–æ–ø–æ–º—ñ–∂–Ω–∏–π –≤–∏–∫–æ–Ω–∞–≤–µ—Ü—å (–∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –º–∞–π–∂–µ –±–µ–∑ –∑–º—ñ–Ω, –∞–ª–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ª–æ–∫–∞–ª—å–Ω—ñ host/user/pwd)
    def _retr(ftp):
        ftp.set_pasv(True)  # –†–µ–∂–∏–º PASV (—è–∫ —É FileZilla)
        ftp.login(user, pwd)
        local_path.parent.mkdir(parents=True, exist_ok=True)
        with open(local_path, "wb") as f:
            ftp.retrbinary(f"RETR {remote_path}", f.write)
        ftp.quit()

    # 1) –°–ø—Ä–æ–±–∞ —á–µ—Ä–µ–∑ Explicit TLS (FTPS) - –±—ñ–ª—å—à –±–µ–∑–ø–µ—á–Ω–æ
    try:
        ftps = ftplib.FTP_TLS(host, timeout=20)
        ftps.auth()
        ftps.prot_p()
        _retr(ftps)
        print(f"[SUCCESS] Downloaded via FTPS: {remote_path}")
        return
    except ftplib.all_errors as e_tls:
        # 2) –Ø–∫—â–æ TLS –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π ‚Äî –ø—Ä–æ–±—É—î–º–æ –∑–≤–∏—á–∞–π–Ω–∏–π FTP
        try:
            print(f"[WARN] FTPS failed for {prefix}, trying plain FTP...")
            ftp = ftplib.FTP(host, timeout=20)
            _retr(ftp)
            print(f"[SUCCESS] Downloaded via FTP: {remote_path}")
            return
        except ftplib.all_errors as e_plain:
            raise RuntimeError(f"FTP/FTPS failed for {prefix}. TLS Error: {e_tls}; Plain Error: {e_plain}")



def unzip_gz_file(gz_file: Path, output_csv: Path) -> None:
    with gzip.open(gz_file, "rb") as f_in, open(output_csv, "wb") as f_out:
        shutil.copyfileobj(f_in, f_out)


# ----------------------- Config helpers -----------------------

def _config_dir() -> Path:
    # backend/app/price_processor.py -> backend/config/...
    return Path(__file__).resolve().parent.parent / "config"


def _load_supplier_cfg(supplier_name: str) -> dict:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–µ–∫—Ü—ñ—é –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –∑ config/suppliers.yaml."""
    cfg_path = _config_dir() / "suppliers.yaml"
    if not cfg_path.exists():
        return {}
    with open(cfg_path, "r", encoding="utf-8") as f:
        all_suppliers = yaml.safe_load(f) or {}
    return (
            all_suppliers.get(supplier_name)
            or all_suppliers.get(supplier_name.upper())
            or all_suppliers.get(supplier_name.lower())
            or {}
    )


# ----------------------- Normalize & parse -----------------------

def _normalize_line_with_cfg(line: str, gt5_to: Optional[int]) -> str:
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä—è–¥–∫–∞ –¥–ª—è ¬´–ø—Ä–æ–±—ñ–ª—å–Ω–∏—Ö¬ª —Ñ–æ—Ä–º–∞—Ç—ñ–≤.
    """
    repl = str(gt5_to if gt5_to is not None else 10)
    line = re.sub(r">\s*5", repl, line)
    m = re.search(r"\w\s\w*\s\w", line)
    if m:
        line = re.sub(r"\s", "", line, count=1)
    line = re.sub(r"\s", ";", line)
    return line


def raw_csv_to_rows(
        input_csv: Path,
        *,
        stock_index: Optional[int],
        stock_header_token: str = "STAN",
        gt5_to: Optional[int] = None,
        skip_rows: int = 0,
        normalize_mode: str = "spaces",
) -> List[List[str]]:
    """
    –ß–∏—Ç–∞—î —Å–∏—Ä–∏–π CSV. –Ø–∫—â–æ stock_index=None, –ø–æ–≤–µ—Ä—Ç–∞—î –≤—Å—ñ —Ä—è–¥–∫–∏ –±–µ–∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –∑–∞–ª–∏—à–∫—ñ–≤.
    """
    rows: List[List[str]] = []

    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ cp1250 –¥–ª—è –ø–æ–ª—å—Å—å–∫–∏—Ö –ø—Ä–∞–π—Å—ñ–≤, —â–æ–± –Ω–µ –±—É–ª–æ –ø–æ–º–∏–ª–æ–∫ –¥–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è
    with open(input_csv, "r", encoding="cp1250", errors="replace") as f:
        for i, raw in enumerate(f):
            if i < skip_rows:
                continue
            raw = raw.strip()
            if not raw:
                continue

            # –†–æ–∑–±–∏–≤–∞—î–º–æ —Ä—è–¥–æ–∫ –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏
            if normalize_mode == "csv":
                parts = raw.split(";")
            else:
                norm = _normalize_line_with_cfg(raw, gt5_to=gt5_to)
                parts = norm.split(";")

            if not parts:
                continue

            # --- –ì–û–õ–û–í–ù–ê –ó–ú–Ü–ù–ê –¢–£–¢ ---
            # –Ø–∫—â–æ –º–∏ –Ω–µ –≤–∫–∞–∑–∞–ª–∏ —ñ–Ω–¥–µ–∫—Å —Å—Ç–æ–∫—É (—è–∫ –¥–ª—è —Ñ–∞–π–ª—É —Ü—ñ–Ω),
            # –º–∏ –ø—Ä–æ—Å—Ç–æ –¥–æ–¥–∞—î–º–æ —Ä—è–¥–æ–∫ —ñ –π–¥–µ–º–æ –¥–∞–ª—ñ, –Ω–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—é—á–∏ —á–∏—Å–ª–∞.
            if stock_index is None:
                rows.append(parts)
                continue

            # --- –õ–û–ì–Ü–ö–ê –î–õ–Ø –§–ê–ô–õ–£ –ó–ê–õ–ò–®–ö–Ü–í (–¥–µ —ñ–Ω–¥–µ–∫—Å –≤–∫–∞–∑–∞–Ω–æ) ---
            idx = stock_index
            if idx < 0 or idx >= len(parts):
                continue

            val = (parts[idx] or "").strip()

            # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∏–ø—É "STAN"
            if val.lower() == (stock_header_token or "").lower():
                continue

            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ '>5'
            if gt5_to is not None and ">" in val:
                val = str(gt5_to)
                parts[idx] = val

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —á–∏—Å–ª–æ (—Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ñ–∞–π–ª—É –∑–∞–ª–∏—à–∫—ñ–≤!)
            try:
                if float(val) <= 0:
                    continue
            except ValueError:
                # –Ø–∫—â–æ –≤ –∫–æ–ª–æ–Ω—Ü—ñ –∑–∞–ª–∏—à–∫—É –Ω–µ —á–∏—Å–ª–æ ‚Äî —ñ–≥–Ω–æ—Ä—É—î–º–æ —Ü–µ–π —Ä—è–¥–æ–∫
                continue

            rows.append(parts)

    return rows


def _rows_to_standard_df(rows: List[List[str]], colmap: Dict[str, int]) -> pd.DataFrame:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º–æ —Å–∏—Ä—ñ —Ä—è–¥–∫–∏ –¥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ—ó –º–æ–¥–µ–ª—ñ –∫–æ–ª–æ–Ω–æ–∫.
    """

    def take(r: List[str], idx: Optional[int]) -> str:
        if idx is None or idx < 0 or idx >= len(r):
            return ""
        return (r[idx] or "").strip()

    data: List[List[Any]] = []
    for r in rows:
        code = take(r, colmap.get("code"))
        unicode_ = take(r, colmap.get("unicode")) or code
        brand = take(r, colmap.get("brand"))
        name = take(r, colmap.get("name")) or brand
        stock_s = take(r, colmap.get("stock"))
        price_s = take(r, colmap.get("price"))

        # stock -> int
        try:
            stock = int(float(stock_s))
        except Exception:
            stock = 0

        # price -> float (–∫–æ–º–∏/–∑–∞–π–≤—ñ —Å–∏–º–≤–æ–ª–∏ –ø—Ä–∏–±–∏—Ä–∞—î–º–æ)
        ps = price_s.replace(",", ".")
        ps = re.sub(r"[^0-9.]", "", ps)
        try:
            price = float(ps)
        except Exception:
            price = float("nan")

        data.append([code, unicode_, brand, name, stock, price])

    df = pd.DataFrame(data, columns=["code", "unicode", "brand", "name", "stock", "price"])
    df["price"] = pd.to_numeric(df["price"], errors="coerce")
    df["stock"] = pd.to_numeric(df["stock"], errors="coerce").fillna(0).astype(int)
    return df


# ----------------------- Pricing & build output -----------------------

def _apply_pricing(
        df: pd.DataFrame,
        factor: float,
        currency_out: str,
        rate: float,
        rounding: Dict[str, int],
) -> pd.Series:
    """
    –û–±—á–∏—Å–ª—é—î —Ñ—ñ–Ω–∞–ª—å–Ω—É —Ü—ñ–Ω—É.
    """
    base = pd.to_numeric(df["price"], errors="coerce").fillna(0.0).astype(float)
    if currency_out.upper() == "UAH":
        val = base * float(factor) * float(rate)
        digits = int(rounding.get("UAH", 0))
    else:
        val = base * float(factor)
        digits = int(rounding.get("EUR", 2))
    return val.round(digits).astype(float)


def _build_output_df(
        df_std: pd.DataFrame,
        price_final: pd.Series,
        columns_cfg: List[Dict[str, str]],
        supplier_id: Optional[int],
) -> pd.DataFrame:
    """
    –ó–±–∏—Ä–∞—î –≤–∏—Ö—ñ–¥–Ω–∏–π DataFrame.
    """
    temp = df_std.copy()
    temp["supplier_id"] = supplier_id if supplier_id is not None else None
    temp["price"] = price_final

    out_cols: Dict[str, pd.Series] = {}
    for col in columns_cfg:
        src = col["from"]
        hdr = col["header"]
        if src not in temp.columns:
            temp[src] = temp.get(src, None)
        out_cols[hdr] = temp[src]

    return pd.DataFrame(out_cols)


# ----------------------- Materialize to CSV -----------------------

def _materialize_to_csv(remote_path: str, tmp_dir: Path, supplier: str) -> tuple[Path, list[Path]]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ñ–∞–π–ª (–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–∫–∞ –∞–±–æ FTP) —Ç–∞ –≥–æ—Ç—É—î –π–æ–≥–æ –¥–æ —á–∏—Ç–∞–Ω–Ω—è.
    –¢–µ–ø–µ—Ä –≤—Ä–∞—Ö–æ–≤—É—î –Ω–∞–∑–≤—É –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ —Ç–∞ —Ç–∏–ø —Ñ–∞–π–ª—É (.csv –∞–±–æ .gz).
    """
    cleanup: list[Path] = []

    # 1) –†–æ–±–æ—Ç–∞ –∑ –ª–æ–∫–∞–ª—å–Ω–∏–º —Ñ–∞–π–ª–æ–º (–¥–ª—è —Ç–µ—Å—Ç—ñ–≤)
    if os.path.exists(remote_path):
        p = Path(remote_path)
        if p.suffix.lower() == ".csv":
            return p, cleanup
        if p.suffix.lower() == ".gz":
            csv_out = tmp_dir / f"{p.stem}.csv"
            unzip_gz_file(p, csv_out)
            cleanup.append(csv_out)
            return csv_out, cleanup
        raise ValueError(f"Unsupported local file type: {p.suffix}")

    # 2) –†–æ–±–æ—Ç–∞ –∑ FTP
    else:
        stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = Path(remote_path).name

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ñ–∞–π–ª –∑–∞–∞—Ä—Ö—ñ–≤–æ–≤–∞–Ω–∏–π
        is_gz = remote_path.lower().endswith(".gz")

        # –°—Ç–≤–æ—Ä—é—î–º–æ —à–ª—è—Ö –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        download_path = tmp_dir / f"ftp_{stamp}_{filename}"

        # --- –í–ò–ö–õ–ò–ö –û–ù–û–í–õ–ï–ù–û–ì–û –ó–ê–í–ê–ù–¢–ê–ñ–£–í–ê–ß–ê ---
        # –ü–µ—Ä–µ–¥–∞—î–º–æ supplier, —â–æ–± —Ñ—É–Ω–∫—Ü—ñ—è –∑–Ω–∞–ª–∞, —è–∫—ñ –ø–∞—Ä–æ–ª—ñ –±—Ä–∞—Ç–∏ –∑ .env
        download_file_from_ftp(remote_path, download_path, supplier)

        if is_gz:
            # –Ø–∫—â–æ —Ü–µ –∞—Ä—Ö—ñ–≤ ‚Äî —Ä–æ–∑–ø–∞–∫–æ–≤—É—î–º–æ
            csv_tmp = download_path.with_suffix(".csv")
            if csv_tmp == download_path:  # –ø—Ä–æ –≤—Å—è–∫ –≤–∏–ø–∞–¥–æ–∫, —â–æ–± –Ω–µ –∑–∞—Ç–µ—Ä—Ç–∏
                csv_tmp = download_path.parent / (download_path.name + "_unzipped.csv")

            unzip_gz_file(download_path, csv_tmp)

            # –î–æ–¥–∞—î–º–æ –æ–±–∏–¥–≤–∞ —Ñ–∞–π–ª–∏ –≤ —á–µ—Ä–≥—É –Ω–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è
            cleanup.extend([download_path, csv_tmp])
            return csv_tmp, cleanup
        else:
            # –Ø–∫—â–æ —Ü–µ –∑–≤–∏—á–∞–π–Ω–∏–π CSV ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –π–æ–≥–æ
            cleanup.append(download_path)
            return download_path, cleanup


# ----------------------- NEW: –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ò–• (–û–î–ò–ù –†–ê–ó) -----------------------

def prepare_base_df(
    supplier: str,
    additional_files: Optional[Dict[str, str]] = None,
    remote_gz_path: Optional[str] = None
) -> Tuple[pd.DataFrame, List[Path]]:
    """
    –£–ù–Ü–í–ï–†–°–ê–õ–¨–ù–ê –ü–Ü–î–ì–û–¢–û–í–ö–ê:
    - –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ñ–∞–π–ª–∏ (–æ–¥–∏–Ω –∞–±–æ –∫—ñ–ª—å–∫–∞).
    - –°—É–º—É—î –∑–∞–ª–∏—à–∫–∏ –ø–æ —Å–∫–ª–∞–¥–∞—Ö (Aggregation).
    - –†–æ–±–∏—Ç—å –º–µ—Ä–¥–∂, —è–∫—â–æ —Ü–µ Autopartner (—Ü—ñ–Ω–∏ + –∑–∞–ª–∏—à–∫–∏).
    - –ü–æ–≤–µ—Ä—Ç–∞—î –≥–æ—Ç–æ–≤–∏–π DataFrame —Ç–∞ —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è.
    """
    tmp_dir = TEMP_DIR
    local_files = {}
    cleanup_paths = []

    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (Download)
    if additional_files:
        print(f"[INFO] üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Ö —Ñ–∞–π–ª—ñ–≤ –¥–ª—è {supplier}...")
        for key, r_path in additional_files.items():
            l_path, c_paths = _materialize_to_csv(r_path, tmp_dir, supplier)
            local_files[key] = l_path
            cleanup_paths.extend(c_paths)
    elif remote_gz_path:
        print(f"[INFO] üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è {supplier}...")
        l_path, c_paths = _materialize_to_csv(remote_gz_path, tmp_dir, supplier)
        local_files["prices"] = l_path
        cleanup_paths.extend(c_paths)

    # 2. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è (Config)
    sup_cfg = _load_supplier_cfg(supplier)
    layout = sup_cfg.get("raw_layout", {}) or {}
    colmap = layout.get("columns") or {}
    read_params = {
        "stock_index": layout.get("stock_index"),
        "stock_header_token": layout.get("stock_header_token", "STAN"),
        "gt5_to": layout.get("gt5_to"),
        "skip_rows": (sup_cfg.get("preprocess") or {}).get("skip_rows", 0),
        "normalize_mode": (sup_cfg.get("normalize") or {}).get("mode", "spaces"),
    }

    # 3. –û–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö
    # –°–¶–ï–ù–ê–†–Ü–ô –ê: Autopartner (2 –æ–∫—Ä–µ–º—ñ —Ñ–∞–π–ª–∏)
    if "prices" in local_files and "stock" in local_files:
        print(f"[INFO] üß© –†–µ–∂–∏–º –ú–ï–†–î–ñ–£ –¥–ª—è {supplier}...")
        rows_p = raw_csv_to_rows(local_files["prices"], **{**read_params, "stock_index": None})
        df_p = _rows_to_standard_df(rows_p, colmap)
        df_p["code"] = df_p["code"].astype(str).str.strip().str.upper()

        rows_s = raw_csv_to_rows(local_files["stock"], **read_params)
        df_s = _rows_to_standard_df(rows_s, colmap)
        df_s["code"] = df_s["code"].astype(str).str.strip().str.upper()

        # --- –°–£–ú–£–Ñ–ú–û –°–ö–õ–ê–î–ò ---
        print(f"[INFO] üîÑ –ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Å—Ç–æ–∫—É: –±—É–ª–æ {len(df_s)} —Ä—è–¥–∫—ñ–≤...")
        df_s = df_s.groupby("code", as_index=False).agg({"stock": "sum"})
        print(f"[INFO] ‚úÖ –ü—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è —Å–∫–ª–∞–¥—ñ–≤: {len(df_s)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∫–æ–¥—ñ–≤.")

        # –ú–µ—Ä–¥–∂ —Ü—ñ–Ω —ñ–∑ —Å—É–º–∞—Ä–Ω–∏–º–∏ –∑–∞–ª–∏—à–∫–∞–º–∏
        df_std = pd.merge(df_p.drop(columns=["stock"]), df_s[["code", "stock"]], on="code", how="inner")

    # –°–¶–ï–ù–ê–†–Ü–ô –ë: –ì–¥–∞–Ω—Å—å–∫ / Maxgear (1 —Ñ–∞–π–ª)
    else:
        print(f"[INFO] üìÑ –†–µ–∂–∏–º –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É –¥–ª—è {supplier}...")
        main_file = local_files.get("prices") or list(local_files.values())[0]
        rows = raw_csv_to_rows(main_file, **read_params)
        df_std = _rows_to_standard_df(rows, colmap)
        df_std["code"] = df_std["code"].astype(str).str.strip().str.upper()

        # –ù–∞–≤—ñ—Ç—å –≤ –æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—ñ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –¥—É–±–ª—ñ (—Ä—ñ–∑–Ω—ñ —Å–∫–ª–∞–¥–∏)
        cols_to_keep = [c for c in df_std.columns if c != 'stock']
        df_std = df_std.groupby(cols_to_keep, as_index=False).agg({"stock": "sum"})

    # 4. –ë—Ä–µ–Ω–¥–∏ (—è–∫—â–æ —î —Ñ–∞–π–ª brands.csv)
    if "brands" in local_files:
        print(f"[INFO] üè∑Ô∏è –î–æ–¥–∞—î–º–æ –ø–æ–≤–Ω—ñ –Ω–∞–∑–≤–∏ –±—Ä–µ–Ω–¥—ñ–≤...")
        df_brands = pd.read_csv(local_files["brands"], sep=";", names=["short_name", "full_name"],
                               encoding="cp1250", quotechar='"', encoding_errors="replace")
        df_brands["short_name"] = df_brands["short_name"].astype(str).str.strip().str.upper()
        df_std["brand"] = df_std["brand"].astype(str).str.strip().str.upper()

        df_std = pd.merge(df_std, df_brands, left_on="brand", right_on="short_name", how="left")
        df_std["brand"] = df_std["full_name"].fillna(df_std["brand"])
        df_std = df_std.drop(columns=["short_name", "full_name"])

    return df_std, cleanup_paths


# ----------------------- Main pipeline -----------------------
def process_one_price(
        df_input: pd.DataFrame,  # –¢–ï–ü–ï–† –ü–†–ò–ô–ú–ê–Ñ –ì–û–¢–û–í–ò–ô DATAFRAME
        supplier: str,
        supplier_id: Optional[int],
        factor: float,
        currency_out: str,  # "EUR" | "UAH"
        format_: str,  # "xlsx" | "csv"
        rounding: Dict[str, int],  # {"EUR":2, "UAH":0}
        r2_prefix: str,  # ".../{supplier}/"
        columns: List[Dict[str, str]],
        csv_cfg: Optional[Dict[str, Any]] = None,
        rate: float = 1.0,
) -> Tuple[str, str]:
    """
    –õ–ï–ì–ö–ò–ô –ï–¢–ê–ü: –¢—ñ–ª—å–∫–∏ –Ω–∞—Ü—ñ–Ω–∫–∞, –∑–∞–ø–∏—Å —É –ë–î —Ç–∞ –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É.
    –ë—ñ–ª—å—à–µ –Ω–µ –∫–∞—á–∞—î FTP —ñ –Ω–µ —Ä–æ–±–∏—Ç—å –º–µ—Ä–¥–∂!
    """
    tmp_dir = TEMP_DIR
    stamp = datetime.now().strftime("%Y%m%d_%H%M")
    supplier_code_str = supplier.lower()

    # 1) –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ø—ñ—é –¥–∞–Ω–∏—Ö –¥–ª—è —Ü—å–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥—É
    # –©–æ–± –Ω–∞—Ü—ñ–Ω–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∞–π—Å—É –Ω–µ –≤–ø–ª–∏–Ω—É–ª–∞ –Ω–∞ —ñ–Ω—à–∏–π
    df_std = df_input.copy()

    # 2) –ö–ê–õ–¨–ö–£–õ–Ø–¶–Ü–Ø –¶–Ü–ù–ò
    price_final = _apply_pricing(
        df_std, factor=factor, currency_out=currency_out, rate=rate, rounding=rounding
    )

    # 3) –ó–ë–Ü–†–ö–ê –í–ò–•–Ü–î–ù–û–ì–û DATAFRAME
    out_df = _build_output_df(
        df_std, price_final, columns_cfg=columns, supplier_id=supplier_id
    )

        # 4) –ó–ê–ü–ò–° –£ POSTGRESQL (—Ç—ñ–ª—å–∫–∏ –¥–ª—è —Å–∞–π—Ç—ñ–≤)
    if "/site/" in r2_prefix and supplier_id is not None:
        try:
            print(f"[INFO] DB Trigger: Updating site prices for ID {supplier_id}...")

            # –ó—á–∏—Ç—É—î–º–æ URL (–ø–µ—Ä–µ–∫–æ–Ω–∞–π—Å—è, —â–æ –≤ .env –≤—ñ–Ω –≤—ñ–¥ Supabase!)
            raw_url = os.getenv("DATABASE_URL")
            if raw_url and raw_url.startswith("postgresql://"):
                db_url = raw_url.replace("postgresql://", "postgresql+psycopg2://", 1)
            else:
                db_url = raw_url or "postgresql+psycopg2://postgres:123456789@localhost:5432/postgres"

            engine = create_engine(db_url, pool_pre_ping=True)

            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ .begin() –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è (COMMIT)
            with engine.begin() as conn:
                # –ö–†–û–ö –ê: –û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –¥–∞–Ω–∏—Ö —Ü—å–æ–≥–æ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞
                conn.execute(
                    text("DELETE FROM product_catalog WHERE supplier_id = :sid"),
                    {"sid": supplier_id}
                )
                print(f"[INFO] DB: Old records for ID {supplier_id} deleted.")

            out_df_db = out_df.replace('\x00', '', regex=True)

            # –î–æ–¥–∞—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
            def _norm_val(v: str) -> str:
                if not v or pd.isna(v): return ""  # –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ –ø–æ—Ä–æ–∂–Ω—ñ—Ö –∫–ª—ñ—Ç–∏–Ω–æ–∫
                return re.sub(r'[^A-Za-z0-9]', '', str(v)).upper()

            if "code" in out_df_db.columns:
                out_df_db["code_norm"] = out_df_db["code"].apply(_norm_val)
            else:
                out_df_db["code_norm"] = None

            if "unicode" in out_df_db.columns:
                out_df_db["unicode_norm"] = out_df_db["unicode"].apply(_norm_val)
            else:
                out_df_db["unicode_norm"] = None

            if "brand" in out_df_db.columns:
                out_df_db["brand_norm"] = out_df_db["brand"].apply(_norm_val)
            else:
                out_df_db["brand_norm"] = None

            out_df_db.to_sql(
                'product_catalog',
                con=engine,
                if_exists='append',
                index=False,
                chunksize=5000
            )

            print(f"[INFO] PostgreSQL: SUCCESS! {len(out_df_db)} items pushed to Supabase.")

        except Exception as e:
            print(f"[ERROR] Database save failed: {e}")

    # 5) –ï–ö–°–ü–û–†–¢ –£ –§–ê–ô–õ (Excel –∞–±–æ CSV)
    ext = "xlsx" if format_.lower() == "xlsx" else "csv"

    # --- üëá –õ–û–ì–Ü–ö–ê –¢–ï–ì–Ü–í –ó–ì–Ü–î–ù–û –ó –¢–í–û–á–ú –ó–ê–ü–ò–¢–û–ú üëá ---
    date_str = datetime.now().strftime("%d.%m.%y")  # 08.02.26
    time_str = datetime.now().strftime("%H%M%S")  # 223005

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –º—ñ—Ç–∫—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–µ—Ñ—ñ–∫—Å–∞ —à–ª—è—Ö—É (r2_prefix)
    prefix_lower = r2_prefix.lower()

    if "exist" in prefix_lower:
        tag = "exist"
    elif "1_23" in prefix_lower:
        tag = "m"
    elif "1_27" in prefix_lower:
        tag = "l"
    elif "site" in prefix_lower or "1_33" in prefix_lower:
        tag = "xl"
    else:
        tag = "data"  # –¢–µ—Ö–Ω—ñ—á–Ω–∏–π —Ç–µ–≥, —è–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –ø—ñ–¥—ñ–π—à–ª–æ

    # –§–æ—Ä–º—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—É –Ω–∞–∑–≤—É (—Ç—ñ–ª—å–∫–∏ –º–∞–ª—ñ –±—É–∫–≤–∏)
    # –ü—Ä–∏–∫–ª–∞–¥: price_autopartner_08.02.26_223005_xl.xlsx
    out_name = f"price_{supplier.lower()}_{date_str}_{time_str}_{tag}.{ext}"
    out_path = tmp_dir / out_name
    # -----------------------------------------------


    if ext == "xlsx":
        out_df.to_excel(out_path, index=False, engine="xlsxwriter")
        content_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    else:
        delim = (csv_cfg or {}).get("delimiter", ";")
        out_df.to_csv(out_path, index=False, sep=delim, header=True, encoding="utf-8")
        content_type = "text/csv"

    # 6) –í–ò–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –í CLOUDFLARE R2
    storage = StorageClient()
    key = f"{r2_prefix}{out_name}"

    url = storage.upload_file(
        local_path=str(out_path),
        key=key,
        content_type=content_type,
        cleanup_prefix=r2_prefix,
        keep_last=5,  # –¢—Ä–∏–º–∞—î–º–æ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –≤–µ—Ä—Å—ñ–π
    )

    # –í–∏–¥–∞–ª—è—î–º–æ –≥–æ—Ç–æ–≤–∏–π Excel/CSV –∑ –¥–∏—Å–∫–∞ –ø—ñ—Å–ª—è –≤–∏–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    out_path.unlink(missing_ok=True)

    return key, url